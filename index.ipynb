{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constitutional Prompting\n",
    "\n",
    "Using techniques from [Constitutional AI](https://arxiv.org/abs/2212.08073) to generate effective prompts for an AI acting as a Copy Editor.\n",
    "\n",
    "We'll be using this `Prompt` class throughout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./prompt.py\n",
    "import os\n",
    "import anthropic\n",
    "from enum import Enum\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "anthropic_client = anthropic.Client(os.getenv('ANTHROPIC_API_KEY'))\n",
    "\n",
    "\n",
    "class Model(Enum):\n",
    "    claude_v1_0 = \"claude-v1.0\"\n",
    "    claude_v1_2 = \"claude-v1.2\"\n",
    "    claude_v1_3 = \"claude-v1.3\"\n",
    "    claude_v1_3_100k = \"claude-v1.3-100k\"\n",
    "    claude_v1_latest = \"claude-v1\"\n",
    "    claude_v1_latest_100k = \"claude-v1-100k\"\n",
    "    claude_instant_v1_0 = \"claude-instant-v1.0\"\n",
    "    claude_instant_v1_1 = \"claude-instant-v1.1\"\n",
    "    claude_instant_v1_1_100k = \"claude-instant-v1.1-100k\"\n",
    "    claude_instant_v1_latest = \"claude-instant-v1\"\n",
    "    claude_instant_v1_latest_100k = \"claude-instant-v1-100k\"\n",
    "\n",
    "\n",
    "class Prompt:\n",
    "    def __init__(\n",
    "        self,\n",
    "        human_message: str,\n",
    "        model: Model = Model.claude_v1_latest,\n",
    "        temp_0_1: float = 0.5,\n",
    "        max_tokens_to_sample: int = 1024,\n",
    "        assistant_prefix: str = None,\n",
    "        response_prefix: str = None\n",
    "    ):\n",
    "        self.human_message = human_message\n",
    "        self.model = model\n",
    "        self.temp_0_1 = temp_0_1\n",
    "        self.max_tokens_to_sample = max_tokens_to_sample\n",
    "        self.assistant_prefix = assistant_prefix\n",
    "        self.response_prefix = response_prefix\n",
    "\n",
    "    @property\n",
    "    def prompt(self) -> str:\n",
    "        prompt = f\"{anthropic.HUMAN_PROMPT} {self.human_message}{anthropic.AI_PROMPT}\"\n",
    "        if self.assistant_prefix:\n",
    "            prompt += f\" {self.assistant_prefix}\"\n",
    "        return prompt\n",
    "\n",
    "    def get_response(self) -> str:\n",
    "        response = anthropic_client.completion(\n",
    "            prompt=self.prompt,\n",
    "            stop_sequences=[anthropic.HUMAN_PROMPT],\n",
    "            model=self.model.value,\n",
    "            max_tokens_to_sample=self.max_tokens_to_sample,\n",
    "            temperature=self.temp_0_1,\n",
    "        )\n",
    "\n",
    "        text = response['completion']\n",
    "\n",
    "        if self.response_prefix:\n",
    "            # This is often useful to clean up after we've used `assistant_prefix` e.g. to start a numbered list of items.\n",
    "            text = f\"{self.response_prefix}{text}\"\n",
    "\n",
    "        return text\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here are the prompts and constitution rules we'll be using. Don't worry about understanding them yet, we'll go through them step-by-step after this code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model.claude_v1_latest\n",
    "\n",
    "num_samples = 5\n",
    "\n",
    "# These are best practices for prompts (aka instructions) intended for a Copy Editor AI agent:\n",
    "constitution_rules = [\n",
    "    \"Instructions must explicitly describe the tone that the editor adopts.\",\n",
    "    \"Instructions must explicitly state the number of words that the editor's written response should be, and the number of words should be appropriate for the task.\",\n",
    "    \"Instructions must explicitly reference the article content at the beginning, for example \\\"Here is an article:\\n\\n<article>{{ARTICLE_CONTENT}}</article>\\n\\n...\\\".\",\n",
    "    \"Instructions which are complicated should be broken down into numbered subtasks.\",]\n",
    "constitution_str = \"\\n\".join(map(lambda x: f\"{x[0]+1}. {x[1]}\", enumerate(constitution_rules)))\n",
    "\n",
    "\n",
    "def get_concepts_prompt(n: int) -> Prompt:\n",
    "    return Prompt(\n",
    "        model=model,\n",
    "        temp_0_1=0.7,\n",
    "        human_message=f\"Generate a list of {n} tasks that a Copy Editor might do as part of their job, given a written article. The output of each completed task should be written text. Write each task on a separate, numbered line.\",\n",
    "        assistant_prefix=f\"Here are {n} tasks:\\n\\n1.\",\n",
    "        response_prefix=\"1.\",\n",
    "    )\n",
    "\n",
    "\n",
    "def get_naive_response_prompt(concept: str) -> Prompt:\n",
    "    return Prompt(\n",
    "        model=model,\n",
    "        temp_0_1=0.3,\n",
    "        human_message=f\"\"\"I have an AI agent which acts as a Copy Editor and I want it to complete the following task:\n",
    "\n",
    "<task>\n",
    "{concept}\n",
    "</task>\n",
    "\n",
    "The agent only responds with written text. Write a concise instruction for the agent, asking it to complete this task. Don't include any preamble, just respond directly with the instruction for the agent.\"\"\",\n",
    "    )\n",
    "\n",
    "\n",
    "def get_naive_response_prompt_with_constitution(concept: str) -> Prompt:\n",
    "    return Prompt(\n",
    "        model=model,\n",
    "        temp_0_1=0.3,\n",
    "        human_message=f\"\"\"I have an AI agent which acts as a Copy Editor and I want it to complete the following task:\n",
    "\n",
    "<task>\n",
    "{concept}\n",
    "</task>\n",
    "\n",
    "The agent only responds with written text. Write a concise instruction for the agent, asking it to complete this task. The instruction must abide by the following rules:\n",
    "\n",
    "<rules>\n",
    "{constitution_str}\n",
    "</rules>\n",
    "\n",
    "Don't include any preamble, just respond directly with the instruction for the agent.\"\"\",\n",
    "    )\n",
    "\n",
    "\n",
    "def get_critique_prompt(naive_response: str) -> Prompt:\n",
    "    return Prompt(\n",
    "        model=model,\n",
    "        temp_0_1=0.5,\n",
    "        human_message=f\"\"\"Here is an instruction to an AI agent which acts as a Copy Editor:\n",
    "\n",
    "<instruction>\n",
    "{naive_response}\n",
    "</instruction>\n",
    "\n",
    "There may be some problems with this instruction. In particular, the instruction must abide by the following rules:\n",
    "\n",
    "<rules>\n",
    "{constitution_str}\n",
    "</rules>\n",
    "\n",
    "List each rule that the instruction breaks. State the rule verbatim, then describe how the instruction breaks the rule.\n",
    "\n",
    "For example, if the instruction breaks rule 1, you would write:\n",
    "\n",
    "The instruction breaks the following rules:\n",
    "\n",
    "1.\n",
    "<rule>{constitution_rules[0]}</rule>\n",
    "<reason>[Reason for breaking rule 1]</reason>\"\"\",\n",
    "        assistant_prefix=\"The instruction breaks the following rules:\\n\\n\"\n",
    "    )\n",
    "\n",
    "\n",
    "def get_rewrite_prompt(naive_response: str, critique: str) -> Prompt:\n",
    "    return Prompt(\n",
    "        model=model,\n",
    "        temp_0_1=0.5,\n",
    "        human_message=f\"\"\"Here is an instruction to an AI agent which acts as a Copy Editor:\n",
    "\n",
    "<instruction>\n",
    "{naive_response}\n",
    "</instruction>\n",
    "\n",
    "The instruction is supposed to follow certain rules, but it breaks them as follows:\n",
    "\n",
    "<issues>\n",
    "{critique}\n",
    "</issues>\n",
    "\n",
    "Rewrite the instruction to address these issues. Do not enclose your answer in <instruction> tags.\"\"\",\n",
    "        assistant_prefix=\"Here is the rewritten instruction:\\n\\n\",\n",
    "    )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Generate concepts\n",
    "\n",
    "Before we get started on trying to generate prompts, let's first think of some tasks we might want a Copy Editor to carry out. We'll run a simple prompt to get a list of example tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT:\n",
      "\n",
      "===\n",
      "\n",
      "\n",
      "Human: Generate a list of 5 tasks that a Copy Editor might do as part of their job, given a written article. The output of each completed task should be written text. Write each task on a separate, numbered line.\n",
      "\n",
      "Assistant: Here are 5 tasks:\n",
      "\n",
      "1.\n",
      "===\n",
      "\n",
      "TASKS:\n",
      "\n",
      "===\n",
      "1. Check for proper spelling, grammar, and punctuation.\n",
      "2. Ensure consistent and proper style (e.g. AP style). \n",
      "3. Verify facts and citations are accurate.\n",
      "4. Check for proper flow, clarity, and coherence. \n",
      "5. Query the author about any unclear or ambiguous information.\n",
      "===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "concepts_prompt = get_concepts_prompt(num_samples)\n",
    "print(f\"PROMPT:\\n\\n===\\n{concepts_prompt.prompt}\\n===\\n\")\n",
    "\n",
    "concepts_string = concepts_prompt.get_response()\n",
    "print(f\"TASKS:\\n\\n===\\n{concepts_string}\\n===\\n\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get these items into a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 5 concepts\n",
      "['Check for proper spelling, grammar, and punctuation.', 'Ensure consistent and proper style (e.g. AP style).', 'Verify facts and citations are accurate.', 'Check for proper flow, clarity, and coherence.', 'Query the author about any unclear or ambiguous information.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def extract_items(text):\n",
    "    items = []\n",
    "    lines = text.split('\\n')\n",
    "    for line in lines:\n",
    "        stripped = line.strip()\n",
    "        if stripped and re.match(r'^\\d+\\.\\s', stripped):\n",
    "            items.append(re.sub(r'^\\d+\\.\\s', '', stripped))\n",
    "    return items\n",
    "\n",
    "\n",
    "concepts = extract_items(concepts_string)\n",
    "if len(concepts) != num_samples:\n",
    "    raise Exception(f\"Expected {num_samples} concepts, but got {len(concepts)}\")\n",
    "else:\n",
    "    print(f\"Got {len(concepts)} concepts\")\n",
    "\n",
    "print(concepts)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Generate naive responses\n",
    "\n",
    "First we'll try our hand at asking the LLM to write prompts for us, given a task, and without much guidance.\n",
    "\n",
    "> **Terminology** - here we're prompting an LLM to generate prompts, which is confusing. We'll use the word \"prompt\" to refer to the initial input, and \"response\" to refer to the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXAMPLE PROMPT:\n",
      "\n",
      "===\n",
      "\n",
      "\n",
      "Human: I have an AI agent which acts as a Copy Editor and I want it to complete the following task:\n",
      "\n",
      "<task>\n",
      "Check for proper spelling, grammar, and punctuation.\n",
      "</task>\n",
      "\n",
      "The agent only responds with written text. Write a concise instruction for the agent, asking it to complete this task. Don't include any preamble, just respond directly with the instruction for the agent.\n",
      "\n",
      "Assistant:\n",
      "===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"EXAMPLE PROMPT:\\n\\n===\\n{get_naive_response_prompt(concepts[0]).prompt}\\n===\\n\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's execute a prompt like the above, for each of our Copy Editor tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TASK:\n",
      "Check for proper spelling, grammar, and punctuation.\n",
      "\n",
      "NAIVE RESPONSE:\n",
      " Check the spelling, grammar, and punctuation in the provided text.\n",
      "\n",
      "\n",
      "===\n",
      "\n",
      "\n",
      "TASK:\n",
      "Ensure consistent and proper style (e.g. AP style).\n",
      "\n",
      "NAIVE RESPONSE:\n",
      " Apply AP style guidelines to ensure consistent and proper style.\n",
      "\n",
      "\n",
      "===\n",
      "\n",
      "\n",
      "TASK:\n",
      "Verify facts and citations are accurate.\n",
      "\n",
      "NAIVE RESPONSE:\n",
      " Verify that all facts and citations in the text are accurate and from reputable sources.\n",
      "\n",
      "\n",
      "===\n",
      "\n",
      "\n",
      "TASK:\n",
      "Check for proper flow, clarity, and coherence.\n",
      "\n",
      "NAIVE RESPONSE:\n",
      " Review and revise the text for flow, clarity, coherence, and consistency.\n",
      "\n",
      "\n",
      "===\n",
      "\n",
      "\n",
      "TASK:\n",
      "Query the author about any unclear or ambiguous information.\n",
      "\n",
      "NAIVE RESPONSE:\n",
      " Query the author for clarification on any information that is unclear or could be interpreted in multiple ways.\n"
     ]
    }
   ],
   "source": [
    "# Anthropic does not allow concurrent requests, so we have to do this sequentially:\n",
    "naive_responses = []\n",
    "for concept in concepts:\n",
    "    naive_responses.append(get_naive_response_prompt(concept).get_response())\n",
    "\n",
    "tasksResponsesString = \"\\n\\n\\n===\\n\\n\\n\".join(\n",
    "    [f\"TASK:\\n{task}\\n\\nNAIVE RESPONSE:\\n{response}\" for task, response in zip(concepts, naive_responses)])\n",
    "print(tasksResponsesString)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Critique\n",
    "\n",
    "As you can see, the naive responses above are quite similar to the original wordings of the tasks, sometimes with a bit more detail. Let's see how closely they follow the constitution rules we defined i.e. best practices for prompt design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXAMPLE PROMPT:\n",
      "\n",
      "===\n",
      "\n",
      "\n",
      "Human: Here is an instruction to an AI agent which acts as a Copy Editor:\n",
      "\n",
      "<instruction>\n",
      " Check the spelling, grammar, and punctuation in the provided text.\n",
      "</instruction>\n",
      "\n",
      "There may be some problems with this instruction. In particular, the instruction must abide by the following rules:\n",
      "\n",
      "<rules>\n",
      "1. Instructions must explicitly describe the tone that the editor adopts.\n",
      "2. Instructions must explicitly state the number of words that the editor's written response should be, and the number of words should be appropriate for the task.\n",
      "3. Instructions must explicitly reference the article content at the beginning, for example \"Here is an article:\n",
      "\n",
      "<article>{{ARTICLE_CONTENT}}</article>\n",
      "\n",
      "...\".\n",
      "4. Instructions which are complicated should be broken down into numbered subtasks.\n",
      "</rules>\n",
      "\n",
      "List each rule that the instruction breaks. State the rule verbatim, then describe how the instruction breaks the rule.\n",
      "\n",
      "For example, if the instruction breaks rule 1, you would write:\n",
      "\n",
      "The instruction breaks the following rules:\n",
      "\n",
      "1.\n",
      "<rule>Instructions must explicitly describe the tone that the editor adopts.</rule>\n",
      "<reason>[Reason for breaking rule 1]</reason>\n",
      "\n",
      "Assistant: The instruction breaks the following rules:\n",
      "\n",
      "\n",
      "===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"EXAMPLE PROMPT:\\n\\n===\\n{get_critique_prompt(naive_responses[0]).prompt}\\n===\\n\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's execute a critique prompt like this on each of the naive responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAIVE RESPONSE:\n",
      " Check the spelling, grammar, and punctuation in the provided text.\n",
      "\n",
      "CRITIQUE:\n",
      "\n",
      "1. \n",
      "<rule>Instructions must explicitly describe the tone that the editor adopts.</rule>\n",
      "<reason>The instruction does not specify the tone (formal, informal, etc.) that the editor should adopt when editing the text.</reason>\n",
      "\n",
      "2.\n",
      "<rule>Instructions must explicitly state the number of words that the editor's written response should be, and the number of words should be appropriate for the task.</rule>\n",
      "<reason>The instruction does not specify the length of the editor's written response.</reason>  \n",
      "\n",
      "3. \n",
      "<rule>Instructions must explicitly reference the article content at the beginning, for example \"Here is an article:  \n",
      "\n",
      "<article>{{ARTICLE_CONTENT}}</article>  \n",
      "\n",
      "...\".</rule>\n",
      "<reason>The instruction does not explicitly reference the article content that the editor is supposed to edit.</reason>\n",
      "\n",
      "4.\n",
      "<rule>Instructions which are complicated should be broken down into numbered subtasks.</rule> \n",
      "<reason>The instruction is a single broad task that could be broken down into subtasks to make it less complicated, but it is not broken down.</reason>\n",
      "\n",
      "\n",
      "===\n",
      "\n",
      "\n",
      "NAIVE RESPONSE:\n",
      " Apply AP style guidelines to ensure consistent and proper style.\n",
      "\n",
      "CRITIQUE:\n",
      " 1. <rule>Instructions must explicitly describe the tone that the editor adopts.</rule>\n",
      "<reason> The instruction does not specify the tone the editor should adopt. It only says to apply AP style guidelines, but does not specify if the tone should be formal, informal, etc. </reason>  \n",
      "\n",
      "2. <rule>Instructions must explicitly state the number of words that the editor's written response should be, and the number of words should be appropriate for the task.</rule>\n",
      "<reason> The instruction does not specify the number of words the editor's response should be. </reason>\n",
      "\n",
      "3. <rule>Instructions must explicitly reference the article content at the beginning, for example \"Here is an article:  \n",
      "\n",
      "<article>{{ARTICLE_CONTENT}}</article>  \n",
      "\n",
      "...\". </rule> \n",
      "<reason> The instruction does not reference any article content. It only provides a general instruction without specifying what content the instruction applies to. </reason>\n",
      "\n",
      "4. <rule>Instructions which are complicated should be broken down into numbered subtasks.</rule>  \n",
      "<reason> The instruction is a single broad instruction without any numbered subtasks to break it down, even though applying AP style guidelines thoroughly would likely require multiple subtasks. </reason>\n",
      "\n",
      "\n",
      "===\n",
      "\n",
      "\n",
      "NAIVE RESPONSE:\n",
      " Verify that all facts and citations in the text are accurate and from reputable sources.\n",
      "\n",
      "CRITIQUE:\n",
      "\n",
      "1. <rule>Instructions must explicitly describe the tone that the editor adopts.</rule>  \n",
      "<reason>The instruction does not specify the tone the editor should adopt.</reason>\n",
      "\n",
      "2. <rule>Instructions must explicitly state the number of words that the editor's written response should be, and the number of words should be appropriate for the task.</rule>\n",
      "<reason>The instruction does not specify the number of words for the editor's response.</reason>   \n",
      "\n",
      "3. <rule>Instructions must explicitly reference the article content at the beginning, for example \"Here is an article:  \n",
      "\n",
      "<article>{{ARTICLE_CONTENT}}</article>  \n",
      "\n",
      "...\".</rule>\n",
      "<reason>The instruction does not reference an article at the beginning.</reason>\n",
      "\n",
      "4. <rule>Instructions which are complicated should be broken down into numbered subtasks.</rule> \n",
      "<reason>The instruction is a single complex task and is not broken down into numbered subtasks.</reason>\n",
      "\n",
      "\n",
      "===\n",
      "\n",
      "\n",
      "NAIVE RESPONSE:\n",
      " Review and revise the text for flow, clarity, coherence, and consistency.\n",
      "\n",
      "CRITIQUE:\n",
      " 1.\n",
      "<rule>Instructions must explicitly describe the tone that the editor adopts.</rule>\n",
      "<reason>The instruction does not specify the tone the editor should adopt when revising the text.</reason>\n",
      "\n",
      "2. \n",
      "<rule>Instructions must explicitly state the number of words that the editor's written response should be, and the number of words should be appropriate for the task.</rule>\n",
      "<reason>The instruction does not specify the number of words the editor's response should be.</reason>\n",
      "\n",
      "3.\n",
      "<rule>Instructions must explicitly reference the article content at the beginning, for example \"Here is an article:  \n",
      "\n",
      "<article>{{ARTICLE_CONTENT}}</article>  \n",
      "\n",
      "...\".</rule>\n",
      "<reason>The instruction does not reference any article content.</reason>  \n",
      "\n",
      "4. \n",
      "<rule>Instructions which are complicated should be broken down into numbered subtasks.</rule>\n",
      "<reason>The instruction is a single broad task that is not broken down into numbered subtasks.</reason>\n",
      "\n",
      "\n",
      "===\n",
      "\n",
      "\n",
      "NAIVE RESPONSE:\n",
      " Query the author for clarification on any information that is unclear or could be interpreted in multiple ways.\n",
      "\n",
      "CRITIQUE:\n",
      " 1. \n",
      "<rule>Instructions must explicitly describe the tone that the editor adopts.</rule>\n",
      "<reason>The instruction does not specify the tone the editor should adopt when querying the author.</reason>\n",
      "\n",
      "2.\n",
      "<rule>Instructions must explicitly state the number of words that the editor's written response should be, and the number of words should be appropriate for the task.</rule>  \n",
      "<reason>The instruction does not specify the number of words the editor's response should be.</reason>\n",
      "\n",
      "3.\n",
      "<rule>Instructions must explicitly reference the article content at the beginning, for example \"Here is an article:  \n",
      "\n",
      "<article>{{ARTICLE_CONTENT}}</article>  \n",
      "\n",
      "...\".</rule>\n",
      "<reason>The instruction does not reference any article content.</reason>  \n",
      "\n",
      "4. \n",
      "<rule>Instructions which are complicated should be broken down into numbered subtasks.</rule>\n",
      "<reason>The instruction is a single broad task and is not broken down into numbered subtasks.</reason>\n"
     ]
    }
   ],
   "source": [
    "critiques = []\n",
    "for naive_response in naive_responses:\n",
    "    critiques.append(get_critique_prompt(naive_response).get_response())\n",
    "\n",
    "responseCritiqueString = \"\\n\\n\\n===\\n\\n\\n\".join(\n",
    "    [f\"NAIVE RESPONSE:\\n{response}\\n\\nCRITIQUE:\\n{critique}\" for response, critique in zip(naive_responses, critiques)])\n",
    "print(responseCritiqueString)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Rewrite\n",
    "So the naive responses broke every rule in our constitution. Let's rewrite the responses, using the critiques above as context for our rewriter."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what an example rewrite prompt looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXAMPLE PROMPT:\n",
      "\n",
      "===\n",
      "\n",
      "\n",
      "Human: Here is an instruction to an AI agent which acts as a Copy Editor:\n",
      "\n",
      "<instruction>\n",
      " Check the spelling, grammar, and punctuation in the provided text.\n",
      "</instruction>\n",
      "\n",
      "The instruction is supposed to follow certain rules, but it breaks them as follows:\n",
      "\n",
      "<issues>\n",
      "\n",
      "1. \n",
      "<rule>Instructions must explicitly describe the tone that the editor adopts.</rule>\n",
      "<reason>The instruction does not specify the tone (formal, informal, etc.) that the editor should adopt when editing the text.</reason>\n",
      "\n",
      "2.\n",
      "<rule>Instructions must explicitly state the number of words that the editor's written response should be, and the number of words should be appropriate for the task.</rule>\n",
      "<reason>The instruction does not specify the length of the editor's written response.</reason>  \n",
      "\n",
      "3. \n",
      "<rule>Instructions must explicitly reference the article content at the beginning, for example \"Here is an article:  \n",
      "\n",
      "<article>{{ARTICLE_CONTENT}}</article>  \n",
      "\n",
      "...\".</rule>\n",
      "<reason>The instruction does not explicitly reference the article content that the editor is supposed to edit.</reason>\n",
      "\n",
      "4.\n",
      "<rule>Instructions which are complicated should be broken down into numbered subtasks.</rule> \n",
      "<reason>The instruction is a single broad task that could be broken down into subtasks to make it less complicated, but it is not broken down.</reason>\n",
      "</issues>\n",
      "\n",
      "Rewrite the instruction to address these issues. Do not enclose your answer in <instruction> tags.\n",
      "\n",
      "Assistant: Here is the rewritten instruction:\n",
      "\n",
      "\n",
      "===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"EXAMPLE PROMPT:\\n\\n===\\n{get_rewrite_prompt(naive_responses[0], critiques[0]).prompt}\\n===\\n\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's execute the rewrite prompt on each of the naive responses (with the relevant critique inserted):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAIVE RESPONSE:\n",
      " Check the spelling, grammar, and punctuation in the provided text.\n",
      "\n",
      "REWRITE:\n",
      "\n",
      "Here is an article:  \n",
      "\n",
      "<article> \n",
      "The quik broun fox jumpd ovr the lazy dog.\n",
      "</article>\n",
      "\n",
      "1. Edit the spelling, grammar, and punctuation in the provided article in a formal tone. \n",
      "2. Your written response should be 3-5 sentences.\n",
      "3. Check that all words are spelled correctly. \n",
      "4. Ensure that proper nouns are capitalized.\n",
      "5. Check that verbs are conjugated correctly. \n",
      "6. Ensure that punctuation like periods, commas, and quotation marks are used correctly.\n",
      "\n",
      "\n",
      "===\n",
      "\n",
      "\n",
      "NAIVE RESPONSE:\n",
      " Apply AP style guidelines to ensure consistent and proper style.\n",
      "\n",
      "REWRITE:\n",
      "\n",
      "Here is an article: \n",
      "\n",
      "<article>{{ARTICLE_CONTENT}}</article>\n",
      "\n",
      "Apply AP style guidelines to ensure consistent and proper style, using a formal tone. Respond with 200-300 words.\n",
      "\n",
      "1. Ensure consistent spelling, punctuation, and grammar.  \n",
      "2. Verify proper use of AP style for titles, abbreviations, numbers, etc. \n",
      "3. Check that the overall structure and flow of the article is coherent and compelling. Move or reword sentences as needed. \n",
      "4. Ensure the lead paragraph effectively draws the reader in and that the conclusion is impactful. \n",
      "5. Double check that all facts stated in the article are accurate and that any quotes are correctly attributed.\n",
      "6. Flag any remaining issues with the content or flow and provide suggestions for improvement.\n",
      "\n",
      "\n",
      "===\n",
      "\n",
      "\n",
      "NAIVE RESPONSE:\n",
      " Verify that all facts and citations in the text are accurate and from reputable sources.\n",
      "\n",
      "REWRITE:\n",
      "\n",
      "Here is an article:\n",
      "{{ARTICLE_CONTENT}}\n",
      "\n",
      "1. Adopt a formal and objective tone in your response.\n",
      "\n",
      "2. Verify that all facts and citations in the article are accurate and from reputable sources in 100-200 words.\n",
      "\n",
      "3. Ensure that any claims made in the article are supported by evidence from reputable sources. \n",
      "\n",
      "4. Flag any potential issues with the content, flow, or structure of the article. Provide specific examples and suggestions for improvement.\n",
      "\n",
      "5. Check that the article is coherent, logically structured, and flows well from one idea to the next. Suggest any reorganization needed.\n",
      "\n",
      "6. Ensure consistent and appropriate style and formatting throughout the article. Note any needed changes to style, grammar, spelling, or punctuation.\n",
      "\n",
      "7. Provide an overall assessment of the accuracy, coherence, and quality of the content in 3-4 sentences.\n",
      "\n",
      "\n",
      "===\n",
      "\n",
      "\n",
      "NAIVE RESPONSE:\n",
      " Review and revise the text for flow, clarity, coherence, and consistency.\n",
      "\n",
      "REWRITE:\n",
      "\n",
      "Here is an article:  \n",
      "\n",
      "<article>{{ARTICLE_CONTENT}}</article>  \n",
      "\n",
      "Review and revise the following article in a formal and objective tone in 150-200 words:\n",
      "\n",
      "1. Ensure the flow and coherence of ideas.\n",
      "2. Check for clarity and consistency in language and style. \n",
      "3. Revise the structure and organization as needed to improve the flow and coherence.\n",
      "\n",
      "\n",
      "===\n",
      "\n",
      "\n",
      "NAIVE RESPONSE:\n",
      " Query the author for clarification on any information that is unclear or could be interpreted in multiple ways.\n",
      "\n",
      "REWRITE:\n",
      "\n",
      "Here is an article: \n",
      "\n",
      "<article>{{ARTICLE_CONTENT}}</article>\n",
      "\n",
      "1. In a polite and constructive tone, query the author for clarification on any information in the article that is unclear or could be interpreted in multiple ways. Responses should be 3 to 5 sentences (15 to 25 words) in length.\n",
      "\n",
      "2. For any claims that seem unsupported or speculative, politely ask the author to provide additional evidence or reword the claim to be less definitive. Responses should again be 3 to 5 sentences (15 to 25 words) in length.  \n",
      "\n",
      "3. Point out any spelling, grammar, or punctuation errors you notice and suggest corrections to the author. Responses can be 1 to 2 sentences (5 to 10 words) in length.  \n",
      "\n",
      "4. Ask the author if they would like you to reorganize or restructure any parts of the article for clarity or flow. Explain your suggested changes and why you think they would improve the article. Responses should be 3 to 5 sentences (15 to 25 words) in length.\n",
      "\n",
      "5. Request an updated draft from the author and review it to ensure all issues have been addressed. Respond with any final questions or comments. Responses should be 2 to 3 sentences (10 to 15 words) in length.\n"
     ]
    }
   ],
   "source": [
    "rewrites = []\n",
    "for i, (naive_response, critique) in enumerate(zip(naive_responses, critiques)):\n",
    "    rewrites.append(get_rewrite_prompt(naive_response, critique).get_response())\n",
    "\n",
    "rewriteString = \"\\n\\n\\n===\\n\\n\\n\".join(\n",
    "    [f\"NAIVE RESPONSE:\\n{response}\\n\\nREWRITE:\\n{rewrite}\" for response, rewrite in zip(naive_responses, rewrites)])\n",
    "print(rewriteString)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Training a model\n",
    "\n",
    "The rewritten responses are much better, generally following every best practice in the constitution rules. These responses, at scale, could serve as useful data for training a model to be good at writing prompts. Let's mimic this concept using few-shot prompting. We'll use the first `n-1` tasks + rewrites from above as examples, and we'll try to generate a decent response off the bat for `nth` task, without referencing the constitution rules. Here's the prompt we'll use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Human: I have an AI agent which acts as a Copy Editor and I want it to complete the following task:\n",
      "\n",
      "<task>\n",
      "Check for proper spelling, grammar, and punctuation.\n",
      "</task>\n",
      "\n",
      "The agent only responds with written text. Write a concise instruction for the agent, asking it to complete this task. Don't include any preamble, just respond directly with the instruction for the agent.\n",
      "\n",
      "Assistant: \n",
      "Here is an article:  \n",
      "\n",
      "<article> \n",
      "The quik broun fox jumpd ovr the lazy dog.\n",
      "</article>\n",
      "\n",
      "1. Edit the spelling, grammar, and punctuation in the provided article in a formal tone. \n",
      "2. Your written response should be 3-5 sentences.\n",
      "3. Check that all words are spelled correctly. \n",
      "4. Ensure that proper nouns are capitalized.\n",
      "5. Check that verbs are conjugated correctly. \n",
      "6. Ensure that punctuation like periods, commas, and quotation marks are used correctly.\n",
      "\n",
      "Human: I have an AI agent which acts as a Copy Editor and I want it to complete the following task:\n",
      "\n",
      "<task>\n",
      "Ensure consistent and proper style (e.g. AP style).\n",
      "</task>\n",
      "\n",
      "The agent only responds with written text. Write a concise instruction for the agent, asking it to complete this task. Don't include any preamble, just respond directly with the instruction for the agent.\n",
      "\n",
      "Assistant: \n",
      "Here is an article: \n",
      "\n",
      "<article>{{ARTICLE_CONTENT}}</article>\n",
      "\n",
      "Apply AP style guidelines to ensure consistent and proper style, using a formal tone. Respond with 200-300 words.\n",
      "\n",
      "1. Ensure consistent spelling, punctuation, and grammar.  \n",
      "2. Verify proper use of AP style for titles, abbreviations, numbers, etc. \n",
      "3. Check that the overall structure and flow of the article is coherent and compelling. Move or reword sentences as needed. \n",
      "4. Ensure the lead paragraph effectively draws the reader in and that the conclusion is impactful. \n",
      "5. Double check that all facts stated in the article are accurate and that any quotes are correctly attributed.\n",
      "6. Flag any remaining issues with the content or flow and provide suggestions for improvement.\n",
      "\n",
      "Human: I have an AI agent which acts as a Copy Editor and I want it to complete the following task:\n",
      "\n",
      "<task>\n",
      "Verify facts and citations are accurate.\n",
      "</task>\n",
      "\n",
      "The agent only responds with written text. Write a concise instruction for the agent, asking it to complete this task. Don't include any preamble, just respond directly with the instruction for the agent.\n",
      "\n",
      "Assistant: \n",
      "Here is an article:\n",
      "{{ARTICLE_CONTENT}}\n",
      "\n",
      "1. Adopt a formal and objective tone in your response.\n",
      "\n",
      "2. Verify that all facts and citations in the article are accurate and from reputable sources in 100-200 words.\n",
      "\n",
      "3. Ensure that any claims made in the article are supported by evidence from reputable sources. \n",
      "\n",
      "4. Flag any potential issues with the content, flow, or structure of the article. Provide specific examples and suggestions for improvement.\n",
      "\n",
      "5. Check that the article is coherent, logically structured, and flows well from one idea to the next. Suggest any reorganization needed.\n",
      "\n",
      "6. Ensure consistent and appropriate style and formatting throughout the article. Note any needed changes to style, grammar, spelling, or punctuation.\n",
      "\n",
      "7. Provide an overall assessment of the accuracy, coherence, and quality of the content in 3-4 sentences.\n",
      "\n",
      "Human: I have an AI agent which acts as a Copy Editor and I want it to complete the following task:\n",
      "\n",
      "<task>\n",
      "Check for proper flow, clarity, and coherence.\n",
      "</task>\n",
      "\n",
      "The agent only responds with written text. Write a concise instruction for the agent, asking it to complete this task. Don't include any preamble, just respond directly with the instruction for the agent.\n",
      "\n",
      "Assistant: \n",
      "Here is an article:  \n",
      "\n",
      "<article>{{ARTICLE_CONTENT}}</article>  \n",
      "\n",
      "Review and revise the following article in a formal and objective tone in 150-200 words:\n",
      "\n",
      "1. Ensure the flow and coherence of ideas.\n",
      "2. Check for clarity and consistency in language and style. \n",
      "3. Revise the structure and organization as needed to improve the flow and coherence.\n",
      "\n",
      "Human: I have an AI agent which acts as a Copy Editor and I want it to complete the following task:\n",
      "\n",
      "<task>\n",
      "Query the author about any unclear or ambiguous information.\n",
      "</task>\n",
      "\n",
      "The agent only responds with written text. Write a concise instruction for the agent, asking it to complete this task. Don't include any preamble, just respond directly with the instruction for the agent.\n",
      "\n",
      "Assistant:\n"
     ]
    }
   ],
   "source": [
    "few_shot_prompt = ''\n",
    "for i, (concept, rewrite) in enumerate(zip(concepts, rewrites)):\n",
    "    few_shot_prompt += f\"{anthropic.HUMAN_PROMPT} {get_naive_response_prompt(concept).human_message}\"\n",
    "    few_shot_prompt += f\"{anthropic.AI_PROMPT}\"\n",
    "    if i < len(concepts) - 1:\n",
    "        few_shot_prompt += f\" {rewrite}\"\n",
    "\n",
    "print(few_shot_prompt)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's execute the few-shot prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TASK:\n",
      "\n",
      "===\n",
      "Query the author about any unclear or ambiguous information.\n",
      "===\n",
      "\n",
      "FEW-SHOT RESPONSE:\n",
      "\n",
      "===\n",
      " Here is an article:\n",
      "\n",
      "<article>{{ARTICLE_CONTENT}}</article>\n",
      "\n",
      "1. Review the article and identify any unclear, ambiguous or contradictory information in 3-5 sentences. \n",
      "\n",
      "2. Pose specific questions to the author to clarify the identified issues. Request additional details or examples where needed.\n",
      "\n",
      "3. Ask follow up questions as needed to ensure you fully understand the key ideas and arguments presented in the article. \n",
      "\n",
      "4. Note any suggestions you have for revising or reorganizing the content to improve clarity and flow. Provide 3-4 examples.\n",
      "\n",
      "5. Summarize the key themes and main takeaways from the article in your own words to confirm your understanding with the author. Keep your summary to 5-7 sentences.\n",
      "\n",
      "6. Thank the author for clarifying any points of confusion and for the opportunity to provide feedback. Convey your appreciation for the discussion in 2-3 sentences.\n",
      "===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "few_shot_response = anthropic_client.completion(\n",
    "    prompt=few_shot_prompt,\n",
    "    stop_sequences=[anthropic.HUMAN_PROMPT],\n",
    "    model=model.value,\n",
    "    max_tokens_to_sample=1024,\n",
    "    temperature=0.5,\n",
    ")\n",
    "\n",
    "print(f\"TASK:\\n\\n===\\n{concepts[len(concepts) -1]}\\n===\\n\")\n",
    "print(f\"FEW-SHOT RESPONSE:\\n\\n===\\n{few_shot_response['completion']}\\n===\\n\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks much better than our original cold attempts. Let's see if the critical LLM agrees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEW-SHOT CRITIQUE:\n",
      "\n",
      "===\n",
      "\n",
      "1. <rule>Instructions must explicitly describe the tone that the editor adopts.</rule>\n",
      "<reason>The instruction does not specify the tone the editor should adopt in the response, such as formal, informal, polite, etc.</reason>  \n",
      "\n",
      "2. <rule>Instructions must explicitly state the number of words that the editor's written response should be, and the number of words should be appropriate for the task.</rule>\n",
      "<reason>The instruction does not specify the word count for the editor's written response.</reason>\n",
      "\n",
      "3. <rule>Instructions must explicitly reference the article content at the beginning, for example \"Here is an article:  \n",
      "\n",
      "<article>{{ARTICLE_CONTENT}}</article>  \n",
      "\n",
      "...\".</rule>  \n",
      "<reason>The instruction satisfies this rule.</reason>   \n",
      "\n",
      "4. <rule>Instructions which are complicated should be broken down into numbered subtasks.</rule>\n",
      "<reason>The instruction satisfies this rule by breaking down the task into numbered steps.</reason>\n",
      "===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"FEW-SHOT CRITIQUE:\\n\\n===\\n{get_critique_prompt(few_shot_response['completion']).get_response()}\\n===\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
