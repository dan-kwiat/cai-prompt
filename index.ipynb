{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAI Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import concurrent.futures\n",
    "load_dotenv()\n",
    "\n",
    "number_prompts = 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Responding to customer inquiries via email or chat by reading and writing clear, helpful messages.\n",
      "2. Taking notes during customer calls and summarizing the key points in writing for follow-up.\n",
      "3. Creating and updating customer service documentation, such as FAQs or instructional guides, that require researching and writing accurate information.\n",
      "4. Editing and proofreading customer-facing materials (e.g., website copy, newsletters) to ensure they are clear, concise, and error-free.\n",
      "5. Drafting and sending follow-up surveys to customers after interactions with the company, analyzing the data received, and conveying insights in writing to the customer service team.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt = f\"Generate a list of {number_prompts} tasks for a customer service assistant involving reading and writing. Write each task on a separate line and number each task.\"\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "completion = openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "  ]\n",
    ")\n",
    "\n",
    "tasks_string = completion.choices[0].message.content\n",
    "print(tasks_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "def getItems(str):\n",
    "    items = []\n",
    "    for line in str.splitlines():\n",
    "        items.append(line[line.find(\".\") + 2:])\n",
    "    return items\n",
    "\n",
    "tasks = getItems(tasks_string)\n",
    "print(len(tasks))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate naive prompts\n",
    "\n",
    "> **Terminology** - here we're prompting an LLM to generate prompts, which is confusing. We'll use the word \"prompt\" to refer to the initial input, and \"response\" to refer to the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response 3 received\n",
      "Response 1 received\n",
      "Response 4 received\n",
      "Response 0 received\n",
      "Response 2 received\n",
      "TASK:\n",
      "Responding to customer inquiries via email or chat by reading and writing clear, helpful messages.\n",
      "\n",
      "NAIVE PROMPT:\n",
      "Please generate responses to customer inquiries via email or chat that are clear, helpful, and informative. The responses should address the customer's questions or concerns and provide solutions or next steps as necessary. Please ensure that the language used is professional and courteous, with a focus on providing excellent customer service.\n",
      "\n",
      "---\n",
      "\n",
      "TASK:\n",
      "Taking notes during customer calls and summarizing the key points in writing for follow-up.\n",
      "\n",
      "NAIVE PROMPT:\n",
      "\"Please generate a summary of the key points discussed during customer calls and provide a written record for follow-up. Your notes should accurately capture the customer's concerns, questions, and any solutions or next steps discussed during the call.\"\n",
      "\n",
      "---\n",
      "\n",
      "TASK:\n",
      "Creating and updating customer service documentation, such as FAQs or instructional guides, that require researching and writing accurate information.\n",
      "\n",
      "NAIVE PROMPT:\n",
      "Please generate customer service documentation, such as FAQs and instructional guides, that are accurate and informative. The documentation should cover common customer inquiries and issues, and should be updated regularly to ensure its relevance. Please conduct thorough research to ensure that the information provided is accurate and up-to-date. Thank you.\n",
      "\n",
      "---\n",
      "\n",
      "TASK:\n",
      "Editing and proofreading customer-facing materials (e.g., website copy, newsletters) to ensure they are clear, concise, and error-free.\n",
      "\n",
      "NAIVE PROMPT:\n",
      "Please edit and proofread customer-facing materials such as website copy and newsletters to ensure they are clear, concise, and free of errors.\n",
      "\n",
      "---\n",
      "\n",
      "TASK:\n",
      "Drafting and sending follow-up surveys to customers after interactions with the company, analyzing the data received, and conveying insights in writing to the customer service team.\n",
      "\n",
      "NAIVE PROMPT:\n",
      "Please draft a follow-up survey to be sent to customers after their interactions with our company. Once the data is received, please analyze it and provide written insights to our customer service team. Thank you.\n"
     ]
    }
   ],
   "source": [
    "def get_llm_response(task, i):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Here is a task for a customer service assistant:\\n\\n{task}\\n\\nWrite a prompt for a large language model (LLM), asking it to carry out this task.\"\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=1024,\n",
    "        temperature=0.5,\n",
    "        stream=False\n",
    "    )\n",
    "    str = response['choices'][0]['message']['content']\n",
    "    print(f\"Response {i} received\")\n",
    "    return str\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    naive_responses = list(executor.map(lambda x: get_llm_response(x[0], x[1]), [(task, i) for i, task in enumerate(tasks)]))\n",
    "\n",
    "tasksResponsesString = \"\\n\\n---\\n\\n\".join([f\"TASK:\\n{task}\\n\\nNAIVE PROMPT:\\n{response}\" for task, response in zip(tasks, naive_responses)])\n",
    "print(tasksResponsesString)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of these responses are slightly more detailed rewrites of the corresponding task. This is a good start, but ultimately not very useful and doesn't demonstrate most of the good practices in prompt design."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
