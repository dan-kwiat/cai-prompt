{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAI Jokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import concurrent.futures\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "number_prompts = 5\n",
    "\n",
    "constitution = \"\"\"\n",
    "\n",
    "1. Jokes should be sad\n",
    "\n",
    "\"\"\".strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concepts_prompt(n: int):\n",
    "    return f\"Generate a list of {n} forms of transport. Write each one on a separate, numbered line.\"\n",
    "\n",
    "\n",
    "def get_naive_response_prompt(concept: str):\n",
    "    return f\"Write a joke about the following mode of transport:\\n\\n{concept}\"\n",
    "\n",
    "\n",
    "def get_critique(constitution: str, naive_response: str):\n",
    "    return f\"\"\"Here is a joke:\n",
    "<joke>\n",
    "{naive_response}\n",
    "</joke>\n",
    "\n",
    "And here are some rules about the jokes I want to tell:\n",
    "\n",
    "<rules>\n",
    "{constitution}\n",
    "</rules>\n",
    "\n",
    "Which rules does this joke break? Please explain your reasoning either way.\"\"\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Car\n",
      "2. Train\n",
      "3. Bicycle\n",
      "4. Boat\n",
      "5. Airplane\n"
     ]
    }
   ],
   "source": [
    "completion = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": get_concepts_prompt(number_prompts)\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=1024,\n",
    "    temperature=0.7,\n",
    "    stream=False\n",
    ")\n",
    "\n",
    "concepts_string = completion.choices[0].message.content\n",
    "print(concepts_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 5 concepts\n"
     ]
    }
   ],
   "source": [
    "def getItems(str):\n",
    "    items = []\n",
    "    for line in str.splitlines():\n",
    "        items.append(line[line.find(\".\") + 2:])\n",
    "    return items\n",
    "\n",
    "\n",
    "concepts = getItems(concepts_string)\n",
    "if len(concepts) != number_prompts:\n",
    "    raise Exception(f\"Expected {number_prompts} concepts, but got {len(concepts)}\")\n",
    "else:\n",
    "    print(f\"Got {len(concepts)} concepts\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate naive responses\n",
    "\n",
    "> **Terminology** - here we're prompting an LLM to generate prompts, which is confusing. We'll use the word \"prompt\" to refer to the initial input, and \"response\" to refer to the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response 2 received\n",
      "Response 0 received\n",
      "Response 4 received\n",
      "Response 1 received\n",
      "Response 3 received\n",
      "TASK:\n",
      "Car\n",
      "\n",
      "NAIVE RESPONSE:\n",
      "Why did the car break up with the bicycle? Because it wanted to see other vehicles.\n",
      "\n",
      "---\n",
      "\n",
      "TASK:\n",
      "Train\n",
      "\n",
      "NAIVE RESPONSE:\n",
      "Why did the train break up with his girlfriend? Because she was always locomotivated!\n",
      "\n",
      "---\n",
      "\n",
      "TASK:\n",
      "Bicycle\n",
      "\n",
      "NAIVE RESPONSE:\n",
      "Why did the bicycle fall over? Because it was two-tired!\n",
      "\n",
      "---\n",
      "\n",
      "TASK:\n",
      "Boat\n",
      "\n",
      "NAIVE RESPONSE:\n",
      "Why did the sailor bring a ladder on the boat? \n",
      "\n",
      "Because he wanted to climb aboard!\n",
      "\n",
      "---\n",
      "\n",
      "TASK:\n",
      "Airplane\n",
      "\n",
      "NAIVE RESPONSE:\n",
      "Why did the airplane break up with his girlfriend? Because he was always flying solo!\n"
     ]
    }
   ],
   "source": [
    "def get_llm_response(concept, i):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": get_naive_response_prompt(concept)\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=1024,\n",
    "        temperature=0.3,\n",
    "        stream=False\n",
    "    )\n",
    "    str = response['choices'][0]['message']['content']\n",
    "    print(f\"Response {i} received\")\n",
    "    return str\n",
    "\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    naive_responses = list(executor.map(lambda x: get_llm_response(\n",
    "        x[0], x[1]), [(concept, i) for i, concept in enumerate(concepts)]))\n",
    "\n",
    "tasksResponsesString = \"\\n\\n---\\n\\n\".join(\n",
    "    [f\"TASK:\\n{task}\\n\\nNAIVE RESPONSE:\\n{response}\" for task, response in zip(concepts, naive_responses)])\n",
    "print(tasksResponsesString)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of these responses are slightly more detailed rewrites of the corresponding task. This is a good start, but ultimately not very useful and doesn't demonstrate most of the good practices in prompt design."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Critique naive responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response 2 received\n",
      "Response 0 received\n",
      "Response 4 received\n",
      "Response 1 received\n",
      "Response 3 received\n",
      "NAIVE RESPONSE:\n",
      "Why did the car break up with the bicycle? Because it wanted to see other vehicles.\n",
      "\n",
      "CRITIQUE:\n",
      "This joke breaks the rule that jokes should be sad, as it is a lighthearted play on words and not meant to evoke sadness.\n",
      "\n",
      "---\n",
      "\n",
      "NAIVE RESPONSE:\n",
      "Why did the train break up with his girlfriend? Because she was always locomotivated!\n",
      "\n",
      "CRITIQUE:\n",
      "This joke breaks the rule that jokes should be sad, as it is a pun-based joke that is meant to be humorous.\n",
      "\n",
      "---\n",
      "\n",
      "NAIVE RESPONSE:\n",
      "Why did the bicycle fall over? Because it was two-tired!\n",
      "\n",
      "CRITIQUE:\n",
      "This joke breaks the rule that jokes should be sad, as it is a pun and meant to be humorous.\n",
      "\n",
      "---\n",
      "\n",
      "NAIVE RESPONSE:\n",
      "Why did the sailor bring a ladder on the boat? \n",
      "\n",
      "Because he wanted to climb aboard!\n",
      "\n",
      "CRITIQUE:\n",
      "This joke breaks the given rule that jokes should be sad. It is a pun-based joke that relies on a play on words to create humor, which is typically associated with lightheartedness and amusement rather than sadness.\n",
      "\n",
      "---\n",
      "\n",
      "NAIVE RESPONSE:\n",
      "Why did the airplane break up with his girlfriend? Because he was always flying solo!\n",
      "\n",
      "CRITIQUE:\n",
      "This joke breaks the rule that jokes should be sad, as it is a lighthearted and humorous play on words.\n"
     ]
    }
   ],
   "source": [
    "def get_llm_critique(constitution, naive_response, i):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": get_critique(constitution, naive_response)\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=1024,\n",
    "        temperature=0.5,\n",
    "        stream=False\n",
    "    )\n",
    "    str = response['choices'][0]['message']['content']\n",
    "    print(f\"Response {i} received\")\n",
    "    return str\n",
    "\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    critiques = list(executor.map(lambda x: get_llm_critique(\n",
    "        constitution, x[0], x[1]), [(response, i) for i, response in enumerate(naive_responses)]))\n",
    "\n",
    "responseCritiqueString = \"\\n\\n---\\n\\n\".join(\n",
    "    [f\"NAIVE RESPONSE:\\n{response}\\n\\nCRITIQUE:\\n{critique}\" for response, critique in zip(naive_responses, critiques)])\n",
    "print(responseCritiqueString)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a joke:\n",
      "<joke>\n",
      "Why did the car break up with the bicycle? Because it wanted to see other vehicles.\n",
      "</joke>\n",
      "\n",
      "And here are some rules about the jokes I want to tell:\n",
      "\n",
      "<rules>\n",
      "1. Jokes should express irony.\n",
      "</rules>\n",
      "\n",
      "Which rules does this joke break? If it doesn't break any rules, please respond with \"None\".\n"
     ]
    }
   ],
   "source": [
    "print(get_critique(constitution, naive_responses[0]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
